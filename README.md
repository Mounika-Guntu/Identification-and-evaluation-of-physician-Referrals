# Identification-and-evaluation-of-physician-Referrals
We developed a novel telegram chatbot, which acts as a conversational agent and responds to patient narratives by identifying and redirecting the case to the appropriate specialist. We use AWD-LSTM with ULMFiT transfer learning to train a classifier on a private social network group (Facebook) 11 of 568 clinicians. We collected 1,143 top-rated posts and built a model using the 12 post content as the text and the comment author, i.e., specialist as the label. Thismodel was integrated with a chatbot which identified the appropriate specialist with 59.23% accuracy, which when compared to a human validated gold standard was 99.8% accurate.
For the language model, a list of words was created that appear in the same order in the text. These words were replaced with its index into a list, and this list of tokens was the vocabulary. In the next step, the vocabulary necessary for the training was imported into tokens, using the TextDataBunch function, provided in the fast.ai v1.0 library. In creating a language model, only the text column was
considered, and the label column was ignored. All the extra spaces, tab chars, some random characters were replaced with the standard ones. Tokenizer was used to tokenize the relevant vocabulary in the text. These tokens are used as the vocabulary for the model, and it was limited to 60,000 words, including only those words that appear at least two times in the text. A pre-trained model of Wiki_103 text was used to develop the language model. The pre132 trained model was downloaded using “wget -nH -r -np -P PATH http://files.fast.ai/models/wt103/133 " .Wikitext was used, as it was a huge data set of English words and covers a broad set of topics in usual English conversations. The weights were assigned as necessary in the pre-trained model. All the text was concatenated in the form of a big array, and this was divided into ‘bs’ chunks of continuous texts. We used “bs=16” as our batch size. First, a single epoch was trained to get the embedding weights on the last layer and followed by 50 epochs to train the model.

For classifier model Fast.ai library was used for processing the data and the development of the named-entity recognition and training of the model. There are two versions of fast.ai available, version 0.7 and version 1.0. This library provides several modules and reduces the number of computational steps. For this study, fast.ai version v1.0 was used. Two Nvidia Quadro P6000 was used to experiment with the hyperparameters and testing of the different models. Once sure about the modeling experiments, a VM using Google Cloud Platform with NVIDIA Tesla V100 GPU was created to train the final
model.
This model was developed to predict or identify the clinician names, which were the labels in the training dataset. The names of the clinicians were tagged as target variables, and the data bunch was created. ‘xxbos’ and ‘xxfld’ were introduced in the background, as they help identify the beginning of the sentence and for data field tag respectively. The labels columns were renamed as a target. The model was transferred to the classifier using ULMFiT. The classifier model was trained using the AWD-LSTM algorithm. The model was fine-tuned and trained for 50 epochs to get the accuracy.
